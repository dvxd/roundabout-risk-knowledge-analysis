#!/usr/bin/env python3
'''
Copyright 2019-2021 Duncan Deveaux

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
'''

import sys
import argparse
import pickle
import numpy as np
from matplotlib.ticker import ScalarFormatter
from matplotlib.gridspec import GridSpec
import matplotlib.pyplot as plt
plt.rcParams['pdf.fonttype'] = 42
plt.rcParams['ps.fonttype'] = 42
sys.path.append('..')
from tools import locations
from tools.ttc_correlation import VariationDataset


def plot_pattern(ttcdata, risk_mode):
    (x, y_ttc, error_ttc) = (np.array(ttcdata.x), np.array(
        ttcdata.ttc_values), np.array(ttcdata.ttc_errors))

    warnings_x = []
    for risk in ttcdata.risk_events[risk_mode]:
        warnings_x.append(risk['time'])

    warnings_y = []
    for _ in warnings_x:
        warnings_y.append(0)
    plt.scatter(
        warnings_x,
        warnings_y,
        label="threshold: {}".format(risk_mode))

    plt.xlabel('Time (x10 seconds)', fontsize='x-small')
    plt.fill_between(
        x,
        y_ttc -
        error_ttc,
        y_ttc +
        error_ttc,
        color='b',
        alpha=0.3)

    plt.plot(
        x,
        y_ttc,
        label='Mean Time To Collision (TTC)',
        color='b',
        linestyle='--',
        marker='o')
    plt.legend()

    plt.title('TTC & TTC Threshold values in a Highway/highD scenario (2x2 lanes)')
    plt.show()


# ................................................................. #
# Plot correlation data from the pickle files generated by parse.py #
# ................................................................. #

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("--location", help="The location ID to analyze.", type=int)
    #parser.add_argument("--input", help="The directory where the pickle files generated by parse.py are located.", default='ttc_parse')
    parser.add_argument(
        "--ttclimit",
        help="The maximal TTC value to be considered to compute TTC values variation, in seconds.",
        type=float,
        default=7.5)
    parser.add_argument(
        "--cvtime",
        help="The amount of TTC data history to be considered when computing the Coefficient of Variation, in seconds.",
        type=float,
        default=450.0)
    argsparse = parser.parse_args()

    input_ids = locations.get_input_for_location(argsparse.location)
    print("Input files for location {}: {}".format(argsparse.location, input_ids))


    # Load pickles
    '''
    noise_ids = {}
    ttc_datas = {}
    for id_str in input_ids:
        ttc_datas[id_str] = {}

        print (id_str)
        timelines = []
        with open('{}/round_ttc_{}.pickle'.format('ttc_parse', id_str), 'rb') as f1:
            result = pickle.load(f1)
            timelines.extend(result['timelines'])

        with open('{}/round_ttc_{}.pickle'.format('ttc_parse_20', id_str), 'rb') as f2:
            result = pickle.load(f2)
            timelines.extend(result['timelines'])


        for (noise_id, (noise, tl)) in enumerate(timelines):

            if noise_id not in noise_ids:
                noise_ids[noise_id] = noise

            framerate = tl.framerate
            averaging_step = 1.0
            time_break = argsparse.cvtime # in seconds

            sub_ttcdata = TTCData.from_ttc_timeline(tl, argsparse.ttclimit, averaging_step, time_break*framerate/averaging_step)
            ttc_datas[id_str][noise_id] = (noise, sub_ttcdata)


    with open('ttc_datas.pickle', 'wb') as handle:
        pickle.dump({'ttc_data': ttc_datas, 'noise_ids':noise_ids}, handle)
    '''


    noise_ids = []
    ttc_datas = []
    with open('ttc_datas.pickle', 'rb') as f1:
        result = pickle.load(f1)
        noise_ids = result['noise_ids']
        ttc_datas = result['ttc_data']

    risks = [1, 2, 3, 4, 5, 6]
    noises = [0.3, 0.5, 1.0, 2.0]

    corrs = {}
    for (noise_id, noise) in noise_ids.items():

        ttc_data = []
        for id_str in input_ids:
            ttc_data.extend(ttc_datas[id_str][noise_id][1])

        if noise not in corrs:
            corrs[noise] = {}

        corrs[noise][noise_id] = VariationDataset(risks)
        for item in ttc_data:
            corrs[noise][noise_id].append_variation(item)


    # COMPUTE BASE CORRS
    ttc_data = []
    for id_str in input_ids:
        ttc_data.extend(ttc_datas[id_str][0][1])

    corrs_base = VariationDataset(risks)
    for item in ttc_data:
        corrs_base.append_variation(item)

    # END COMPUTE BASE CORRS


    # Plotting with location error
    risk_title = ['Number of risky events vs. TTC Variation',
                  'Accumulated TET vs. TTC Variation',
                  'Exit probability-weighted accumulated TET vs. TTC Variation']

    fig = plt.figure(constrained_layout=False)
    gs = GridSpec(1, 3, figure=fig)
    for grid_j in range(3):
        risk_id = grid_j
        ax = fig.add_subplot(gs[0, grid_j])

        # Histogram init
        n_bins = len(risks)
        mse_vals = []
        mse_errors = []
        r2_vals = []
        r2_errors = []
        r2_noiseless = []

        for risk_mode in risks:

            (xbase, ybase) = corrs_base.get_scatter_x_y(risk_id, risk_mode)
            n = xbase.size

            ((slope_0, intercept_0), r2_0) = corrs_base.get_regression_line(
                risk_id, risk_mode)
            model_base = np.polyval([slope_0, intercept_0], xbase)

            RMSE_base = 1.0 / n * np.sum((ybase - model_base)**2)
            RMSE_base = np.sqrt(RMSE_base)

            mse_val_current = []
            mse_errors_current = []
            r2_val_current = []
            r2_errors_current = []
            for noise in noises:

                mse_values_series = []
                r2_values_series = []
                for noise_id in corrs[noise].keys():

                    ((slope_n, intercept_n), r2_n) = corrs[noise][noise_id].get_regression_line(
                        risk_id, risk_mode)
                    model_n = np.polyval([slope_n, intercept_n], xbase)

                    RMSE_n = 1.0 / n * np.sum((ybase - model_n)**2)
                    RMSE_n = np.sqrt(RMSE_n)

                    RMSE_diff = ((RMSE_n - RMSE_base) / RMSE_base) * 100.0

                    mse_values_series.append(RMSE_diff)
                    r2_values_series.append(r2_n)

                #print (mse_values_series)
                print('Mean Added RMSE for risk {} and noise {}:{}'.format(
                    risk_mode, noise, np.mean(mse_values_series)))
                mse_val_current.append(np.mean(mse_values_series))
                mse_errors_current.append(
                    1.729 *
                    np.std(mse_values_series) /
                    np.sqrt(
                        len(mse_values_series)))

                r2_val_current.append(np.mean(r2_values_series))
                r2_errors_current.append(
                    1.729 *
                    np.std(r2_values_series) /
                    np.sqrt(
                        len(r2_values_series)))

            mse_vals.append(np.array(mse_val_current))
            mse_errors.append(np.array(mse_errors_current))

            r2_vals.append(np.array(r2_val_current))
            r2_errors.append(np.array(r2_errors_current))
            r2_noiseless.append(r2_0)

        mse_vals = np.array(mse_vals)
        mse_errors = np.array(mse_errors)
        r2_vals = np.array(r2_vals)
        r2_errors = np.array(r2_errors)

        x = np.arange(n_bins)

        bar_width = 0.2
        capsize = 2
        ax.bar(x - 0.3,
               mse_vals[:,
                        0],
               bar_width,
               yerr=mse_errors[:,
                               0],
               capsize=capsize,
               color='mediumseagreen')
        ax.bar(x - 0.1,
               mse_vals[:,
                        1],
               bar_width,
               yerr=mse_errors[:,
                               1],
               capsize=capsize,
               color='yellowgreen')
        ax.bar(x + 0.1, mse_vals[:, 2], bar_width,
               yerr=mse_errors[:, 2], capsize=capsize, color='khaki')
        ax.bar(x + 0.3,
               mse_vals[:,
                        3],
               bar_width,
               yerr=mse_errors[:,
                               3],
               capsize=capsize,
               color='darkorange')

        ax.set_yscale('log')
        ax.yaxis.set_major_formatter(ScalarFormatter())

        #ax2 = ax.twinx()
        # ax2.set_ylim((0.0,1.15))
        #color_r2 = 'steelblue'
        #ax2.set_ylabel('Coefficient of Determination r²', color=color_r2)
        #ax2.tick_params(axis='y', labelcolor=color_r2)
        #for bin_ix in range(n_bins):
            #x_ = [
            #    x[bin_ix] - 0.3,
            #    x[bin_ix] - 0.1,
            #    x[bin_ix] + 0.1,
            #    x[bin_ix] + 0.3]
            #y_ = r2_vals[bin_ix, :]
            #ax2.errorbar(x_, y_, yerr=r2_errors[bin_ix,:], markerfacecolor=color_r2, capsize=1, color='steelblue')

            # ax2.annotate('Errorless\nr² =
            # {}'.format(np.round(r2_noiseless[bin_ix],2)), (x_[1] + 0.1,
            # np.max(y_) + 0.12), backgroundcolor='#ffffffAA', color=color_r2,
            # ha='center', va='center')

        ticks = ['0']
        ticks.extend(["{}s".format(risk_mode) for risk_mode in risks])

        ax.set_xticklabels(ticks)
        ax.set_xlabel('Risk TTC Threshold (s)')
        ax.set_ylabel('Root-Mean-Square-Error increase (%)')
        ax.set_title(risk_title[risk_id])

    fig.legend(["Positioning error STD: {}m".format(noise_)
               for noise_ in noises], ncol=len(noises), loc='lower center')
    fig.suptitle(
        "Impact of Localization Error on TTC Variation-based Risk Prediction",
        fontsize=15)

    fig.tight_layout()
    fig.subplots_adjust(bottom=0.22)

    plt.show()

    #corr_analysis.plot_scatter("Relationship between the CV of TTC Values and Various Levels of Risk (rounD)")
    #corr_analysis.plot_risk_comparison("Relationship Between TTC Variation and Defined Risk Metrics, for a TTC Threshold of " + r"$\bf{3}$" + " " + r"$\bf{seconds}$" + ".", risk_mode=3)
    #corr_analysis.plot_correlation("Correlation between the CV of TTC Values and Various Levels of Risk (rounD)")


    # Plotting with location errorr
    risk_mode = 2

    fig = plt.figure(constrained_layout=True)
    gs = GridSpec(1, 3, figure=fig)
    for grid_j in range(3):
        risk_id = grid_j
        ax = fig.add_subplot(gs[0, grid_j])

        for noise in noises:
            if noise == 1.0:

                key_val = list(corrs[noise].keys())[0]
                corrs[noise][key_val].plot_risk_comparison(
                    "Relationship Between TTC Variation and Defined Risk Metrics, for a TTC Threshold of 2s.",
                    risk_mode=2)

        ax.legend()
        # ax.set_ylim(bottom=0)

    fig.suptitle(
        "Localization error impact for a TTC Threshold of {}s".format(risk_mode),
        fontsize=15)
    plt.show()
